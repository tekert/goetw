//go:build windows

package etw

import (
	"context"
	"errors"
	"fmt"
	"reflect"
	"runtime"
	"sync"
	"sync/atomic"
	"syscall"
	"time"
	"unsafe"

	plog "github.com/phuslu/log"
)

var (
	// rtLostEventGuid is the GUID for RT_LostEvent notifications that indicate real-time event loss.
	// https://learn.microsoft.com/en-us/windows/win32/etw/rt-lostevent
	rtLostEventGuid = &GUID{ /* {6A399AE0-4BC6-4DE9-870B-3657F8947E7E}*/
		Data1: 0x6a399ae0,
		Data2: 0x4bc6,
		Data3: 0x4de9,
		Data4: [8]byte{0x87, 0x0b, 0x36, 0x57, 0xf8, 0x94, 0x7e, 0x7e},
	}

	// EventTraceGuid is used to identify a event tracing session
	EventTraceGuid = &GUID{ /* 68fdd900-4a3e-11d1-84f4-0000f80464e3 */
		Data1: 0x68fdd900,
		Data2: 0x4a3e,
		Data3: 0x11d1,
		Data4: [8]byte{0x84, 0xf4, 0x00, 0x00, 0xf8, 0x04, 0x64, 0xe3},
	}
)

// There 4 places where to get Lost events:
// 1. [Consumer Callback] From the RT_LostEvent event type.
// 2. [Consumer BufferCallback] From the LogfileInfo.EventsLost (seems it's not always populated).
// 3. [Consumer BufferCallback] From the LogfileInfo.LogfileHeader.EventsLost (only for ETL files).
// 4. [Session side] From the QueryTrace() call, which returns the total number of lost events

// SessionSlice converts a slice of structures implementing Session
// to a slice of Session.
func SessionSlice(i any) (out []Session) {
	v := reflect.ValueOf(i)

	switch v.Kind() {
	case reflect.Slice:
		out = make([]Session, 0, v.Len())
		for i := 0; i < v.Len(); i++ {
			if s, ok := v.Index(i).Interface().(Session); ok {
				out = append(out, s)
				continue
			}
			panic("slice item must implement Session interface")
		}
	default:
		panic("interface parameter must be []Session")
	}

	return
}

// Once Consumer is closed discard it and create a new one.
//
// Performance Note on "Consumer-Side" Filtering:
// Any filtering performed within the callbacks of a Consumer (e.g., EventRecordCallback,
// EventCallback) is the least performant method. At this stage, the event has already
// been generated by the provider, sent to the ETW runtime, passed all runtime filters,
// and delivered to the consumer session buffer. The full performance cost of the event
// has already been paid.
//
// Therefore, consumer-side filtering should be used for complex post-processing logic,
// not for high-volume event reduction where performance is critical. For performance,
// always prefer provider-side (Level/Keyword) or runtime-side (ProviderFilter) filtering.
type Consumer struct {
	sync.WaitGroup
	ctx     context.Context
	cancel  context.CancelFunc
	closed  bool
	started bool // True if the consumer has been started.

	tmu sync.RWMutex // Protect trace updates

	// Holds references to consumer currently active traces.
	// safe to hold a reference to ConsumerTrace after closing the consumer.
	traces sync.Map // map[string]*ConsumerTrace

	lastError atomic.Value // stores error

	// [1] First callback executed, it allows to filter out events
	// based on fields of raw ETW EventRecord structure. When this callback
	// returns true event processing will continue, otherwise it is aborted.
	// Filtering out events here has the lowest overhead on the consumer side, but the
	// full cost of event generation and delivery has already been incurred.
	//
	// NOTE: The timestamp in
	// EventRecord.EventHeader.TimeStamp is the raw timestamp and its in FILETIME
	// format only if PROCESS_TRACE_MODE_RAW_TIMESTAMP is not set.
	// if PROCESS_TRACE_MODE_RAW_TIMESTAMP is set, use the EventRecordHelperCallback
	// on later callbacks to access the converted timestamp via EventRecordHelper.Timestamp().
	EventRecordCallback func(*EventRecord) bool

	// [2] Callback which executes after TraceEventInfo is parsed.
	// To filter out some events call Skip method of EventRecordHelper
	// As Properties are not parsed yet, trying to get/set Properties is
	// not possible and might cause unexpected behaviours.
	// errors returned by this callback will be logged.
	// To skip further processing of the event set EventRecordHelper.Flags.Skip = true
	EventRecordHelperCallback func(*EventRecordHelper) error

	// [3] Callback executed after event properties got prepared (step before parsing).
	// Properties are not parsed yet and this is the right place to filter
	// events based only on some properties.
	// NB:Â events skipped in EventRecordCallback never reach this function
	// errors returned by this callback will be logged.
	// To skip further processing of the event set EventRecordHelper.Flags.Skip = true
	EventPreparedCallback func(*EventRecordHelper) error

	// [4] Callback executed after the event got parsed and defines what to do
	// with the event (printed, sent to a channel ...)
	//
	// NOTE: Remember to call e.Release() after processing the event.
	// to improve performance when using this type of processing.
	//
	// Errors returned by this callback are logged but do not stop processing.
	EventCallback func(*Event) error

	// LostEvents tracks the total number of events lost across all trace sessions.
	// This is incremented based on RT_LostEvent notifications.
	LostEvents atomic.Uint64

	// // Skipped tracks the total number of events that were filtered out by callbacks.
	// // Events are counted as skipped when any callback returns false or calls Skip().
	// Skipped atomic.Uint64

	// closeTimeout specifies the maximum time to wait for ProcessTrace to complete
	// when closing the Consumer. If ProcessTrace doesn't return within this timeout,
	// the Consumer will attempt to force termination.
	closeTimeout time.Duration

	// Events provides a buffered channel interface for batch event processing.
	// When configured, events are queued in batches rather than processed individually
	// through callbacks, allowing for more efficient bulk processing patterns.
	Events *EventBuffer
}

// traceContext holds the context information passed to ETW callbacks.
// This structure bridges the gap between the C callback environment and Go,
// providing access to the Consumer, ConsumerTrace, and thread-local storage.
type traceContext struct {
	trace    *ConsumerTrace
	consumer *Consumer
	storage  *traceStorage // thread local storage to reduce allocations and lock contention.
}

// Helper function to get the traceContext from the UserContext
// These are used from ETW callbacks to get a reference back to our context.
// NOTE: keep the pointer alive
func (er *EventRecord) userContext() *traceContext {
	return (*traceContext)(unsafe.Pointer(er.UserContext))
}
func (e *EventTraceLogfile) getContext() *traceContext {
	return (*traceContext)(unsafe.Pointer(e.Context))
}

// NewConsumer creates a new Consumer to consume ETW
func NewConsumer(ctx context.Context) (c *Consumer) {
	c = &Consumer{
		Events: NewEventBuffer(),
	}

	c.ctx, c.cancel = context.WithCancel(ctx)
	// DefaultEventRecordHelperCallback is removed as filtering is now handled by EnableProvider.
	// Users can set their own callback for custom post-collection filtering if needed.
	c.EventRecordHelperCallback = nil
	c.EventCallback = c.DefaultEventCallback

	return c
}

// SetTraceInfoCache enables or disables the caching of the TRACE_EVENT_INFO schema buffer.
// Disabling this forces a call to TdhGetEventInformation for every event, which is
// useful for debugging but will impact performance.
//
// This function is not thread-safe and must be called before the consumer is started.
func (c *Consumer) SetTraceInfoCache(enable bool) error {
	if c.started { // TODO: and (not closed) when we do that a consumer can be reused?
		return errors.New("cannot change trace info cache setting after consumer has started")
	}
	globalTraceEventInfoCacheEnabled = enable
	return nil
}

// gets or creates a new one if it doesn't exist
func (c *Consumer) getOrAddTrace(traceName string) *ConsumerTrace {
	actual, _ := c.traces.LoadOrStore(traceName, newConsumerTrace(traceName))
	return actual.(*ConsumerTrace)
}

// GetTraces returns a snapshot of all managed ConsumerTrace objects with statistics.
//
//	traces := consumer.GetTraces()
//	for name, trace := range traces {
//	    lostEvents := trace.RTLostEvents.Load()
//	    errorEvents := trace.ErrorEvents.Load()
//	    fmt.Printf("Trace %s: lost=%d, errors=%d\n", name, lostEvents, errorEvents)
//	}
func (c *Consumer) GetTraces() map[string]*ConsumerTrace {
	traces := make(map[string]*ConsumerTrace)
	// create map from sync.Map
	c.traces.Range(func(key, value any) bool {
		t := value.(*ConsumerTrace)
		c.tmu.RLock()
		tc := t
		c.tmu.RUnlock()
		traces[key.(string)] = tc
		return true
	})
	return traces
}

// GetTrace retrieves a pointer to the live ConsumerTrace object by its name.
// This allows access to real-time statistics via the exported atomic fields.
// It returns the trace and a boolean indicating whether it was found.
func (c *Consumer) GetTrace(tname string) (t *ConsumerTrace, ok bool) {
	v, ok := c.traces.Load(tname)
	if !ok {
		return nil, false
	}
	return v.(*ConsumerTrace), true
}

// reportError is the centralized error handler for the consumer callback.
func (c *Consumer) reportError(err error, er *EventRecord) {
	var parseErr *ParseError

	switch {
	// Log full details for specific errors.
	case errors.Is(err, ErrGetEventInformation):
		conlog.SampledErrorWithErrSig("GetEventInformation", err).
			Interface("eventRecord", er).
			Msg("Failed to get trace event info")

	case errors.Is(err, ErrPropertyParsingTdh):
		entry := conlog.SampledErrorWithErrSig("formatprop-tdh", err)
		if errors.As(err, &parseErr) {
			entry.Interface("property", parseErr.p).
				Interface("property", parseErr.p.traceInfo).
				Msg("tdh failed to format property (ParseError)")
		} else {
			entry.Msg("tdh failed to format property")
		}

	// Default for any other generic error.
	default:
		entry := conlog.SampledErrorWithErrSig("callback-error", err)
		if conlog.Logger.Level <= plog.DebugLevel {
			// Debug level: Log the full EventRecord using its detailed MarshalJSON.
			entry.Interface("eventRecord", er)
		} else {
			if conlog.Logger.Level >= plog.ErrorLevel {
				entry.Str("EventRecord", fmt.Sprintf("%+v", er))
			}
		}
		entry.Msg("Consumer callback error")
	}

	c.lastError.Store(err)
}

// https://learn.microsoft.com/en-us/windows/win32/etw/rt-lostevent
// EventType{32, 33, 34}, EventTypeName{"RTLostEvent", "RTLostBuffer", "RTLostFile"}]
func (c *Consumer) handleLostEvent(e *EventRecord) {
	var traceInfo *TraceEventInfo
	var err error
	var buffer []byte

	traceInfo, err = e.GetEventInformation(&buffer)
	if err != nil {
		conlog.Error().Err(err).Msg("Failed to get event information for lost event")
	}

	ctx := e.userContext()
	if ctx != nil && ctx.trace != nil && err == nil {
		switch traceInfo.EventDescriptor.Opcode {
		case 32:
			// The RTLostEvent event type indicates that one or more events were lost.
			ctx.trace.RTLostEvents.Add(1)
		case 33:
			// The RTLostBuffer event type indicates that one or more buffers were lost
			ctx.trace.RTLostBuffer.Add(1)
		case 34:
			// The RTLostFile indicates that the backing file used by the AutoLogger
			// to capture events was lost.
			ctx.trace.RTLostFile.Add(1)
		default:
			conlog.Debug().Uint8("opcode", traceInfo.EventDescriptor.Opcode).
				Msg("Invalid opcode for lost event")
		}
	}
	c.LostEvents.Add(1)
}

// https://learn.microsoft.com/en-us/windows/win32/api/evntrace/nc-evntrace-pevent_trace_buffer_callbacka
//
// ETW event consumers implement this function to receive statistics about each buffer
// of events that ETW delivers during a trace processing Session.
// ETW calls this function after the events for each Trace Session buffer are delivered.
// The pointer received is the same on each subsequent call for each trace.
// The pointer received here is not the same one used in OpenTrace.
func (c *Consumer) bufferCallback(e *EventTraceLogfile) uintptr {
	// ensure userctx is not garbage collected after CloseTrace or it crashes invalid mem.
	userctx := e.getContext()

	if userctx == nil || c.ctx.Err() != nil || userctx.trace.ctx.Err() != nil {
		// if the consumer or a trace has been stopped we
		// don't process event records anymore
		// return 0 to stop ProcessTrace
		conlog.SampledTrace("bufferCallback").Msg("Context canceled, stopping ProcessTrace via BufferCallback.")
		return 0 // Returning 0 terminates ProcessTrace.
	}

	if userctx.trace.open {
		userctx.trace.updateTraceLogFile(e)
	}

	// we keep processing event records
	return 1
}

// https://learn.microsoft.com/en-us/windows/win32/api/evntrace/nc-evntrace-pevent_record_callback
//
// Called when ProcessTrace gets an event record.
// This is always called from the same thread as ProcessTrace,
// so it may block other events in the same trace session buffer until it returns.
// Only one event record is received at a time by each ProcessTrace thread.
// This callback can be executed concurrently only if there are multiple
// ProcessTrace goroutines with this callback set.
func (c *Consumer) callback(er *EventRecord) (re uintptr) {
	// Count the number of events with errors, but only once per event.
	setError := func(err error) {
		er.userContext().trace.ErrorEvents.Add(1)
		c.reportError(err, er)
		c.lastError.Store(err) // TODO: no longer useful?
	}

	// We must always get the context first. just in case. if the context is there
	// then everything inside is valid while ProcessTrace is running.
	usrCtx := er.userContext()
	if usrCtx == nil {
		setError(fmt.Errorf("EventRecord has no UserContext, skipping event"))
		return
	}

	// Skips the event if it is the event trace header. Log files contain this event
	// but real-time sessions do not. The event contains the same information as
	// the EVENT_TRACE_LOGFILE.LogfileHeader member that you can access when you open
	// the trace.
	if !usrCtx.trace.realtime {
		if er.EventHeader.ProviderId.Equals(EventTraceGuid) &&
			er.EventHeader.EventDescriptor.Opcode == EVENT_TRACE_TYPE_INFO {
			conlog.Debug().Interface("event", er).Msg("Skipping EventTraceGuid event")
			// Skip this event.
			return
		}
	}

	if er.EventHeader.ProviderId.Equals(rtLostEventGuid) {
		c.handleLostEvent(er)
		return
	}

	// calling EventHeaderCallback if possible
	if c.EventRecordCallback != nil {
		if !c.EventRecordCallback(er) {
			return
		}
	}

	// we get the TraceContext from EventRecord.UserContext
	// Parse TRACE_EVENT_INFO from the event record
	if h, err := newEventRecordHelper(er); err == nil {

		// Convert EventRecord timestamp to FILETIME based on Session
		// ClientContext settings (Controller side where the trace is).
		// Does nothing if PROCESS_TRACE_MODE_RAW_TIMESTAMP (Consumer side) is not set.
		// If that flag is not set, ETW already converts it to FILETIME format.
		if usrCtx.trace.processTraceMode&PROCESS_TRACE_MODE_RAW_TIMESTAMP != 0 {
			h.timestamp = usrCtx.trace.fromRawTimestamp(er.EventHeader.TimeStamp)
		} else {
			h.timestamp = er.EventHeader.TimeStamp
		}

		// return mem to pool when done (big performance improvement)
		// only releases non nil allocations so it's safe to use before h.initialize.
		defer h.release()

		if c.EventRecordHelperCallback != nil {
			if err = c.EventRecordHelperCallback(h); err != nil {
				setError(fmt.Errorf("EventRecordHelperCallback failed: %w", err))
			}
		}
		// if event must be skipped we do not further process it
		if h.Flags.Skip {
			return
		}
		// Event if it's not skipped, return if no further processing is needed
		if c.EventPreparedCallback == nil && c.EventCallback == nil {
			return
		}

		// initialize record helper
		h.initialize()

		// Use TraceInfo to prepare properties without parsing the values yet.
		if err := h.prepareProperties(); err != nil {
			setError(fmt.Errorf("prepareProperties failed: %w", err))
			return
		}

		// The user can parse properties in this callback by using EventRecordHelper.Get*
		if c.EventPreparedCallback != nil {
			if err := c.EventPreparedCallback(h); err != nil {
				setError(fmt.Errorf("EventPreparedCallback failed: %w", err))
			}
		}
		// check if we must skip event after next hook
		if h.Flags.Skip || c.EventCallback == nil {
			return
		}

		var event *Event
		if event, err = h.buildEvent(); err != nil {
			setError(fmt.Errorf("buildEvent failed: %w", err))
		}
		if err := c.EventCallback(event); err != nil {
			setError(fmt.Errorf("EventCallback failed: %w", err))
		}
	} else {
		setError(fmt.Errorf("newEventRecordHelper failed: %w", err))
	}

	return
}

// stopTrace handles the shutdown logic for a single trace. It is the definitive
// function for stopping a trace, ensuring thread safety and proper resource cleanup.
func (c *Consumer) stopTrace(name string, timeout time.Duration) error {
	v, ok := c.traces.Load(name)
	if !ok {
		return nil // Not found, already closed.
	}
	trace := v.(*ConsumerTrace)

	// If not processing, it's already stopped or never started.
	if !trace.processing.Load() {
		return nil
	}

	// Set the timeout that processTraceWithTimeout will use.
	trace.closeTimeout = timeout

	// 1: Signal the context. This provides a fast path for the
	// BufferCallback to stop processing new events.
	trace.cancel()

	// 2: Signal the ProcessTrace function directly via the Windows API.
	// This guarantees that an idle ProcessTrace call will unblock.
	c.closeTrace(trace)

	// 3: Wait for the supervisor goroutine to finish. This will block
	// until the trace has finished gracefully, timed out, or been aborted.
	<-trace.done

	// 4: Perform the final cleanup of the trace from the consumer's map.
	c.cleanupTrace(trace, name)

	return nil
}

// cleanupTrace performs the final cleanup of a trace, deciding whether to
// delete it from the map immediately or spawn a reaper for a detached goroutine.
func (c *Consumer) cleanupTrace(trace *ConsumerTrace, name string) {
	// This check is performed *after* waiting for the goroutine to complete.
	if !trace.processing.Load() {
		// The goroutine finished gracefully. It is now safe to remove the trace
		// from the map.
		c.traces.Delete(name)
		seslog.Debug().Str("trace", name).Msg("Trace closed and removed from consumer.")
	} else {
		// The goroutine was detached (due to timeout or abort). We must not
		// delete it from the map yet. Spawn a reaper goroutine to perform
		// the cleanup once the underlying ProcessTrace call finally returns.
		seslog.Warn().Str("trace", name).Msg("Trace goroutine was detached; spawning reaper for eventual cleanup.")
		go func() {
			ticker := time.NewTicker(1 * time.Second) // go 1.23+ collects these.
			for range ticker.C {
				if !trace.processing.Load() {
					// The detached goroutine has finally finished. It is now safe
					// to remove the trace from the map.
					c.traces.Delete(name)
					seslog.Debug().Str("trace", name).Msg("Reaped and cleaned up detached trace.")
					return // Exit the reaper goroutine.
				}
			}
		}()
	}
}

// closeTrace is the minimal, thread-safe helper to call the Windows CloseTrace API.
// It only signals the handle to close and does not manage map cleanup.
func (c *Consumer) closeTrace(trace *ConsumerTrace) (err error) {
	c.tmu.Lock()
	defer c.tmu.Unlock()

	if trace.handle == 0 || !trace.open {
		return nil
	}

	// The handle will be considered invalid and closed after this point,
	// regardless of the API call's return value.
	defer func() {
		trace.handle = 0
		trace.open = false
	}()

	// if we don't wait for traces, ERROR_CTX_CLOSE_PENDING is a valid error
	// The ERROR_CTX_CLOSE_PENDING code indicates that the CloseTrace function
	// call was successful; the ProcessTrace function will unblock and return
	// after the etw buffer for the session is empty and the last callback returns.
	// (ProcessTrace will not receive any new events in it's buffer after you call
	// the CloseTrace function).
	seslog.Debug().Str("trace", trace.TraceName).Msg("Closing handle for trace")
	err = CloseTrace(trace.handle)
	if err != nil && err != ERROR_CTX_CLOSE_PENDING {
		seslog.Error().Err(err).Str("trace", trace.TraceName).Msg("CloseTrace API call failed")
		return err // Return only on a true error.
	}

	return nil // Return nil on success or pending close.
}

// closeAll closes all open handles and eventually waits for ProcessTrace goroutines
// to return if wait = true
func (c *Consumer) closeAll(wait bool) (lastErr error) {
	if c.closed {
		seslog.Debug().Msg("Consumer already closed.")
		return
	}
	seslog.Debug().Msg("Closing all traces for consumer...")

	// 1: Signal the global context for all callbacks.
	c.cancel()

	// 2: Signal all traces to unblock by calling closeTrace for each one.
	c.traces.Range(func(key, value any) bool {
		trace := value.(*ConsumerTrace)
		if err := c.closeTrace(trace); err != nil {
			lastErr = err // Capture the last real error encountered.
		}
		return true
	})

	// 3: If requested, perform a single, global wait for all goroutines to finish.
	if wait {
		seslog.Debug().Msg("Waiting for all ProcessTrace goroutines to end...")
		c.Wait()
		seslog.Debug().Msg("All ProcessTrace goroutines ended.")
	}

	// 4: After waiting, iterate again to perform the final cleanup for each trace.
	c.traces.Range(func(key, value any) bool {
		name := key.(string)
		trace := value.(*ConsumerTrace)
		c.cleanupTrace(trace, name)
		return true
	})

	c.Events.close()

	// After all processing is complete, flush the global sampler to report
	// a summary of suppressed logs that occurred during the consumer's lifetime.
	GetLogManager().GetSampler().Flush()

	c.closed = true

	return
}

// TODO(tekert): implement function to set ProcessTraceMode flags before consumer is started.

// OpenTrace opens a Trace for consumption.
// All traces are opened with the new PROCESS_TRACE_MODE_EVENT_RECORD flag
func (c *Consumer) OpenTrace(name string) (err error) {
	c.tmu.Lock()
	defer c.tmu.Unlock()

	var traceHandle syscall.Handle
	ti := c.getOrAddTrace(name)
	// Important: we must keep a Go reference, so we save this first to ti._ctx
	// to prevent being garbage collected
	ti._ctx = &traceContext{
		trace:    ti,
		consumer: c,
		storage:  newTraceStorage(),
	}

	// https://learn.microsoft.com/en-us/windows/win32/api/evntrace/ns-evntrace-event_trace_logfilea
	// Create a temporary EventTraceLogfile struct. It will be populated by OpenTrace on success.
	var loggerInfo EventTraceLogfile
	// PROCESS_TRACE_MODE_EVENT_RECORD to receive EventRecords (new format)
	// PROCESS_TRACE_MODE_RAW_TIMESTAMP don't convert TimeStamp member of EVENT_HEADER and EVENT_TRACE_HEADER to system time
	// PROCESS_TRACE_MODE_REAL_TIME to receive events in real time
	var tracemode uint32 = PROCESS_TRACE_MODE_EVENT_RECORD | PROCESS_TRACE_MODE_RAW_TIMESTAMP
	if ti.realtime {
		tracemode = tracemode | PROCESS_TRACE_MODE_REAL_TIME
	}
	// NOTE: for ETL the Union (ProcessTraceMode | LogFileMode) member is set to LogFileMode:
	// https://learn.microsoft.com/en-us/windows/win32/etw/logging-mode-constants
	loggerInfo.SetProcessTraceMode(tracemode)
	loggerInfo.BufferCallback = syscall.NewCallbackCDecl(c.bufferCallback)
	loggerInfo.Callback = syscall.NewCallbackCDecl(c.callback)
	loggerInfo.Context = uintptr(unsafe.Pointer(ti._ctx))
	// Use memory allocated already in the trace struct for the name
	if !ti.realtime {
		loggerInfo.LogFileName = ti.TraceNameW // We consume from the file.
	} else {
		// We use the session name to open the trace
		loggerInfo.LoggerName = ti.TraceNameW
	}

	if traceHandle, err = OpenTrace(&loggerInfo); err != nil {
		return err
	}

	// Trace open
	ti.handle = syscall.Handle(traceHandle)
	ti.open = true

	// On success, OpenTrace populates loggerInfo with the initial trace state.
	// We perform a one-time deep copy to initialize our internal state safely.
	// Since pointers may not be valid when the trace is closed,
	// we clone the EventTraceLogfile structure (except the pointers).
	ti.traceLogfile = *loggerInfo.Clone()

	// NOTE: we save the tracemode we used to open the trace to know if we should handle raw timestamps for each trace callback.
	// ! For ETL file traces the returned ProcessTraceMode is overwrriten with the logfile.LogfileHeader.LogFileMode.
	// ! If we dont set this, timestamps using raw mode will not be converted by this library when consuming from ETL file traces.
	ti.processTraceMode = tracemode
	//ti.processTraceMode = loggerInfo.GetProcessTraceMode() // this works fine for real time traces but not for ETL files.

	// Cache the bootime for lock-free access in the event callback.
	ti.bootTime = loggerInfo.LogfileHeader.BootTime

	// Determine and store the clock type from the log file header.
	// The ReservedFlags field indicates the clock resolution used by the session.
	ti.ClockType = ClockType(ti.traceLogfile.LogfileHeader.ReservedFlags)
	seslog.Info().Str("trace", name).Str("ClockType", ti.ClockType.String()).Msg("Consumer opened trace")

	return nil
}

// same as [ConsumerTrace.QueryTrace] but using string traceName
func (c *Consumer) QueryTrace(traceName string) (prop *EventTraceProperties2Wrapper, err error) {
	value, ok := c.traces.Load(traceName)
	if !ok {
		return nil, fmt.Errorf("trace %s not found", traceName)
	}
	t := value.(*ConsumerTrace)
	return t.QueryTrace()
}

// FromSessions initializes the consumer from sessions
func (c *Consumer) FromSessions(sessions ...Session) *Consumer {
	for _, s := range sessions {
		c.getOrAddTrace(s.TraceName())
	}

	return c
}

// FromTraceNames initializes consumer from existing traces
func (c *Consumer) FromTraceNames(names ...string) *Consumer {
	for _, n := range names {
		c.getOrAddTrace(n)
	}
	return c
}

// inneficient. (i almost rewrote every single function in this fork...)
// DefaultEventCallback is the default [Consumer.EventCallback] method applied
// to Consumer created with NewConsumer
// Receives parsed events and sends them to the Consumer.Events channel.
//
// func (c *Consumer) DefaultEventCallback_old(event *Event) (err error) {
// 	// we have to check again here as the lock introduced delay
// 	if c.ctx.Err() == nil {
// 		// if the event can be skipped we send it in a non-blocking way
// 		if event.Flags.Skippable {
// 			select {
// 			case c.Events <- event:
// 			default:
// 				c.Skipped.Add(1)
// 				event.Release()
// 			}
// 			return
// 		}
// 		// if we cannot skip event we send it in a blocking way
// 		c.Events <- event
// 	}
// 	return
// }

// Will only work while using the [DefaultEventCallback] method.
// This is an example of how to read events with channels, 6% overhead
// vs reading events from custom EventCallback function
//
// ProcessEvents processes events from the Consumer.EventsBatch channel.
// This function blocks.
// The function fn is called for each event.
// To cancel it just call [Consumer.Stop()] or close the context.
//
// For the func(*Event) error: return err to unblock.
//
//	go func() {
//		c.ProcessEvents(func(e *etw.Event) {
//			_ = e
//		})
//	}()
//
// or
//
//	err := c.ProcessEvents(func(e *Event) error {
//	    _ = e
//	   if someCondition {
//	       return fmt.Errorf("error") // stops processing
//	   }
//	   return nil // continues processing
//	})
func (c *Consumer) ProcessEvents(fn any) error {
	switch cb := fn.(type) {
	case func(*Event):
		// Simple callback without return
		for batch := range c.Events.Channel {
			for _, e := range batch {
				cb(e)
				e.Release()
			}
		}
	case func(*Event) error:
		// Callback with bool return to control flow
		for batch := range c.Events.Channel {
			for _, e := range batch {
				if err := cb(e); err != nil {
					e.Release()
					return err
				}
				e.Release()
			}
		}
	}
	return nil
}

// DefaultEventCallback is the default [Consumer.EventCallback] method applied
// to Consumer created with NewConsumer
// Receives parsed events and sends them in batches to the Consumer.EventsBatch channel.
//
// Sends event in batches to the Consumer.EventsBatch channel. (better performance)
// After 200ms have passed or after 20 events have been queued by default.
func (c *Consumer) DefaultEventCallback(event *Event) error {
	// Check context before sending to avoid sending events during shutdown.
	if c.ctx.Err() == nil {
		c.Events.Send(event) // blocks if channel is full.
	}
	return nil
}

// Start starts the consumer, for each real time trace opened starts ProcessTrace in new goroutine
// Also opens any trace session not already opened for consumption.
func (c *Consumer) Start() (err error) {
	c.started = true
	// opening all traces that are not opened first,
	c.traces.Range(func(key, value any) bool {
		name := key.(string)
		trace := value.(*ConsumerTrace)
		// if trace is already opened skip
		if trace.open {
			return true // continue iteration
		}

		if err = c.OpenTrace(name); err != nil {
			err = fmt.Errorf("failed to open trace %s: %w", name, err)
			return false // stop iteration
		}
		return true
	})
	if err != nil {
		return err
	}

	// It waits for all trace processing to finish
	// and then ensures the event channel is closed.
	go func() {
		c.Wait()         // Block until all ProcessTrace goroutines call Done().
		c.Events.close() // Safely close the channel.
	}()

	// It listens for an explicit shutdown signal (e.g., Ctrl+C)
	// and ensures the event channel is closed to unblock the user's
	// ProcessEvents loop. (no effect is using the other callbacks 1,2,3)
	go func() {
		<-c.ctx.Done()   // Block until the consumer's context is canceled.
		c.Events.close() // Safely close the channel.
	}()

	// opens a new goroutine for each trace and blocks.
	c.traces.Range(func(key, value any) bool {
		name := key.(string)
		trace := value.(*ConsumerTrace)
		if trace.processing.Load() {
			return true // continue iteration
		}

		c.Add(1)
		go func(name string, trace *ConsumerTrace) {
			defer c.Done()
			defer close(trace.done) // Signal completion when this goroutine exits.

			//c.processTrace(name, trace)
			c.processTraceWithTimeout(name, trace)
		}(name, trace)
		return true
	})

	return
}

func (c *Consumer) processTrace(name string, trace *ConsumerTrace) {
	// Lock the goroutine to the OS thread (callback will also be an os thread)
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()

	goroutineID := getGoroutineID()
	seslog.Info().Str("trace", name).Interface("goroutineID", goroutineID).Msg("Starting processing trace")

	trace.processing.Store(true)
	trace.StartTime = time.Now()
	// https://docs.microsoft.com/en-us/windows/win32/api/evntrace/nf-evntrace-processtrace
	// IMPORTANT:
	// Won't return even if canceled (CloseTrace or callbackBuffer returning 0),
	// until the session buffer is empty, meaning the defined callback has to return
	// for every remaining event in the buffer, then ProcessTrace will unblock.
	// This must be to make sure all events are processed before  the user closes the handle.
	if err := ProcessTrace(&trace.handle, 1, nil, nil); err != nil {
		if err == ERROR_CANCELLED {
			// The consumer canceled processing by returning FALSE in their
			// BufferCallback function.
			seslog.Info().Str("trace", name).Err(err).Msg("ProcessTrace canceled")
		} else {
			lastErr := fmt.Errorf(
				"ProcessTrace failed: %w, handle: %v, LoggerName: %s", err, trace.handle, name)
			c.lastError.Store(lastErr)
			seslog.Error().Err(lastErr).Msg("ProcessTrace failed")
		}
	}
	trace.processing.Store(false)
	trace._ctx = nil // context can be safely released now. (bufferCallback will not be called anymore)
	seslog.Debug().Str("trace", name).Msg("ProcessTrace finished")
}

// Will return from the goroutine if ProcessTrace doesn't return after close.
// This will leave the ProcessTrace detached and flushing events.
func (c *Consumer) processTraceWithTimeout(name string, trace *ConsumerTrace) {
	pdone := make(chan struct{})
	// Start ProcessTrace in separate goroutine
	go func() {
		defer close(pdone)
		c.processTrace(name, trace) // blocks until ProcessTrace returns
	}()

	var timeout time.Duration

	// Wait for one of three conditions:
	// 1. Global consumer context is canceled (`c.ctx.Done()`).
	// 2. This specific trace's context is canceled (`trace.ctx.Done()`).
	// 3. The underlying ProcessTrace function finishes on its own (`pdone`).
	select {
	case <-c.ctx.Done(): // Global shutdown signal.
		seslog.Debug().Str("trace", name).Msg("Global context canceled, shutting down trace...")
		timeout = c.closeTimeout
	case <-trace.ctx.Done(): // Individual shutdown signal.
		seslog.Debug().Str("trace", name).Msg("Trace context canceled, shutting down trace...")
		timeout = trace.closeTimeout
	case <-pdone: // ProcessTrace finished cleanly.
		return // Finished cleanly before any cancellation signal.
	}

	// If we are here, it means a cancellation (interrupt) occurred. We now handle the shutdown.
	switch {
	case timeout > 0: // Wait with a timeout.
		timer := time.NewTimer(timeout)
		defer timer.Stop()

		select {
		case <-pdone:
			// ProcessTrace returned gracefully before timeout.
		case <-timer.C:
			seslog.Warn().Str("trace", name).Msg("ProcessTrace did not return within timeout, detaching goroutine.")
		}
	case timeout == 0: // Wait indefinitely.
		<-pdone
	default: // (Abort), don't wait at all.
		// Just return, detaching the goroutine.
	}
}

// LastError returns the last error encountered by the consumer
func (c *Consumer) LastError() error {
	if v := c.lastError.Load(); v != nil {
		return v.(error)
	}
	return nil
}

// Stop initiates a graceful shutdown of the consumer.
//
// It performs the following steps:
//  1. Signals all trace processing loops to stop.
//  2. Calls CloseTrace for each open trace handle, which prevents new events from being delivered.
//  3. Waits indefinitely for the ProcessTrace function to finish processing all events
//     remaining in the ETW buffers.
//
// This is the safest method to ensure no events are lost. It blocks until the shutdown is complete.
func (c *Consumer) Stop() (err error) {
	// ProcessTrace will exit only if the session buffer is empty.
	c.closeTimeout = 0
	return c.closeAll(true)
}

// StopWithTimeout initiates a shutdown with a timeout.
//
// It follows the same graceful shutdown sequence as Stop, but if the ProcessTrace
// function for any trace does not return within the specified timeout, the consumer
// will stop waiting for it and return. The underlying goroutine for the timed-out
// ProcessTrace call will be detached and left to finish on its own.
//
// This is useful for preventing an indefinite block if an ETW session is unresponsive.
//
// Special timeout values:
//   - timeout > 0: Waits for the specified duration.
//   - timeout = 0: Behaves exactly like Stop(), waiting indefinitely.
//   - timeout < 0: Behaves exactly like Abort(), returning immediately.
func (c *Consumer) StopWithTimeout(timeout time.Duration) (err error) {
	// ProcessTrace will exit only if the session buffer is empty.
	c.closeTimeout = timeout
	return c.closeAll(true)
}

// Abort forces an immediate, non-graceful shutdown of the consumer.
//
// It calls CloseTrace to signal the end of consumption but does *not* wait for
// the ProcessTrace goroutines to finish processing buffered events. The underlying
// goroutines are detached and may continue running in the background until they
// naturally complete.
//
// Use this method when an immediate return is required and the potential loss of
// in-flight events is acceptable.
func (c *Consumer) Abort() (err error) {
	c.closeTimeout = -1 // A negative value signals an immediate return.
	return c.closeAll(false)
}

// StopTrace gracefully shuts down a single trace session by name, with an optional timeout.
//
// It signals the specific trace's processing loop to stop, calls the underlying
// CloseTrace API function, and waits for the processing to complete, respecting the
// provided timeout. Any buffered events for that trace will be processed before it returns.
//
// If a timeout is provided and expires, the function will return, but the underlying
// processing goroutine will be detached and left to finish on its own.
//
// This function is safe for concurrent use.
func (c *Consumer) StopTrace(name string, timeout ...time.Duration) error {
	var t time.Duration = 0 // Default to wait indefinitely.
	if len(timeout) > 0 {
		t = timeout[0]
	}
	return c.stopTrace(name, t)
}
